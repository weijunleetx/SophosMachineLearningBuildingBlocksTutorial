C:\Tutorial>dir /s *.py *.csv
 Volume in drive C is Acer
 Volume Serial Number is 2872-7314

 Directory of C:\Tutorial

2018-06-29  06:56 AM            11,362 urlmodel-new.py
               1 File(s)         11,362 bytes

 Directory of C:\Tutorial\data-new

2018-06-24  04:14 PM        59,647,721 clean-new.csv
2018-06-24  05:32 PM         3,731,738 dirty-new.csv
               2 File(s)     63,379,459 bytes


C:\Tutorial>python urlmodel-new.py compare --filepath data-new --n 100000
Using TensorFlow backend.
06/29/2018 07:24:39 AM __main__     INFO     Importing clean URLs
06/29/2018 07:24:41 AM __main__     INFO     Importing dirty URLs
06/29/2018 07:24:41 AM __main__     INFO     Max dirty urls = 62286
06/29/2018 07:24:41 AM __main__     INFO     Dirty samples: 62286
06/29/2018 07:24:41 AM __main__     INFO     Clean samples: 436002
06/29/2018 07:24:48 AM __main__     INFO     MMH3-hashing feature vectors...
06/29/2018 07:26:29 AM __main__     INFO     MMH3-hashed data samples: 498288
06/29/2018 07:27:58 AM __main__     INFO
06/29/2018 07:27:59 AM __main__     INFO     Try a shallow model with time-split data ...
06/29/2018 07:27:59 AM __main__     INFO     Constructing model - shallow
06/29/2018 07:28:01 AM __main__     INFO     Constructed model:
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 1)                 3001
=================================================================
Total params: 3,001
Trainable params: 3,001
Non-trainable params: 0
_________________________________________________________________
06/29/2018 07:28:01 AM __main__     INFO     Start training model...
Epoch 1/20
326034/326034 [==============================] - 27s 82us/step - loss: 0.3729 - acc: 0.8751
Epoch 2/20
326034/326034 [==============================] - 8s 25us/step - loss: 0.3250 - acc: 0.8829
Epoch 3/20
326034/326034 [==============================] - 7s 23us/step - loss: 0.3139 - acc: 0.8858
Epoch 4/20
326034/326034 [==============================] - 7s 23us/step - loss: 0.3065 - acc: 0.8868
Epoch 5/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.3008 - acc: 0.8880
Epoch 6/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2963 - acc: 0.8894
Epoch 7/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2926 - acc: 0.8913
Epoch 8/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2895 - acc: 0.8928
Epoch 9/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2869 - acc: 0.8944
Epoch 10/20
326034/326034 [==============================] - 7s 21us/step - loss: 0.2846 - acc: 0.8957
Epoch 11/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2826 - acc: 0.8965
Epoch 12/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2809 - acc: 0.8972
Epoch 13/20
326034/326034 [==============================] - 7s 23us/step - loss: 0.2794 - acc: 0.8976
Epoch 14/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2780 - acc: 0.8982
Epoch 15/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2768 - acc: 0.8985
Epoch 16/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2756 - acc: 0.8994
Epoch 17/20
326034/326034 [==============================] - 7s 22us/step - loss: 0.2746 - acc: 0.9017
Epoch 18/20
326034/326034 [==============================] - 7s 23us/step - loss: 0.2737 - acc: 0.9023
Epoch 19/20
326034/326034 [==============================] - 7s 23us/step - loss: 0.2728 - acc: 0.9028
Epoch 20/20
326034/326034 [==============================] - 8s 23us/step - loss: 0.2720 - acc: 0.9036
06/29/2018 07:30:47 AM __main__     INFO     Predicting test dataset... shallow
06/29/2018 07:30:58 AM __main__     INFO     AUC = [0.7563579753331555]
06/29/2018 07:30:58 AM __main__     INFO     Plotting results for shallow model @ data-new\shallowmodel_timesplit
C:\Program Files\Python36\lib\site-packages\matplotlib\tight_layout.py:207: UserWarning: tight_layout : falling back to Agg renderer
  warnings.warn("tight_layout : falling back to Agg renderer")
06/29/2018 07:31:00 AM __main__     INFO
06/29/2018 07:31:00 AM __main__     INFO     Try a deep model with time-split data ...
06/29/2018 07:31:00 AM __main__     INFO     Constructing model - deep
06/29/2018 07:31:01 AM __main__     INFO     Constructed model:
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_2 (Dense)              (None, 128)               384128
_________________________________________________________________
batch_normalization_1 (Batch (None, 128)               512
_________________________________________________________________
activation_1 (Activation)    (None, 128)               0
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0
_________________________________________________________________
dense_3 (Dense)              (None, 128)               16512
_________________________________________________________________
batch_normalization_2 (Batch (None, 128)               512
_________________________________________________________________
activation_2 (Activation)    (None, 128)               0
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0
_________________________________________________________________
dense_4 (Dense)              (None, 64)                8256
_________________________________________________________________
batch_normalization_3 (Batch (None, 64)                256
_________________________________________________________________
activation_3 (Activation)    (None, 64)                0
_________________________________________________________________
dropout_3 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_5 (Dense)              (None, 64)                4160
_________________________________________________________________
batch_normalization_4 (Batch (None, 64)                256
_________________________________________________________________
activation_4 (Activation)    (None, 64)                0
_________________________________________________________________
dropout_4 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_6 (Dense)              (None, 32)                2080
_________________________________________________________________
batch_normalization_5 (Batch (None, 32)                128
_________________________________________________________________
activation_5 (Activation)    (None, 32)                0
_________________________________________________________________
dropout_5 (Dropout)          (None, 32)                0
_________________________________________________________________
dense_7 (Dense)              (None, 32)                1056
_________________________________________________________________
batch_normalization_6 (Batch (None, 32)                128
_________________________________________________________________
activation_6 (Activation)    (None, 32)                0
_________________________________________________________________
dropout_6 (Dropout)          (None, 32)                0
_________________________________________________________________
dense_8 (Dense)              (None, 1)                 33
=================================================================
Total params: 418,017
Trainable params: 417,121
Non-trainable params: 896
_________________________________________________________________
06/29/2018 07:31:01 AM __main__     INFO     Start training model...
Epoch 1/20
326034/326034 [==============================] - 46s 140us/step - loss: 0.3482 - acc: 0.8806
Epoch 2/20
326034/326034 [==============================] - 34s 103us/step - loss: 0.2894 - acc: 0.8994
Epoch 3/20
326034/326034 [==============================] - 33s 103us/step - loss: 0.2675 - acc: 0.9081
Epoch 4/20
326034/326034 [==============================] - 33s 102us/step - loss: 0.2547 - acc: 0.9126
Epoch 5/20
326034/326034 [==============================] - 33s 101us/step - loss: 0.2438 - acc: 0.9164
Epoch 6/20
326034/326034 [==============================] - 33s 100us/step - loss: 0.2349 - acc: 0.9196
Epoch 7/20
326034/326034 [==============================] - 34s 106us/step - loss: 0.2272 - acc: 0.9222
Epoch 8/20
326034/326034 [==============================] - 33s 102us/step - loss: 0.2199 - acc: 0.9247
Epoch 9/20
326034/326034 [==============================] - 33s 102us/step - loss: 0.2125 - acc: 0.9269
Epoch 10/20
326034/326034 [==============================] - 34s 103us/step - loss: 0.2068 - acc: 0.9289
Epoch 11/20
326034/326034 [==============================] - 34s 104us/step - loss: 0.2007 - acc: 0.9313
Epoch 12/20
326034/326034 [==============================] - 35s 106us/step - loss: 0.1950 - acc: 0.9331
Epoch 13/20
326034/326034 [==============================] - 35s 106us/step - loss: 0.1889 - acc: 0.9352
Epoch 14/20
326034/326034 [==============================] - 35s 107us/step - loss: 0.1830 - acc: 0.9370
Epoch 15/20
326034/326034 [==============================] - 35s 107us/step - loss: 0.1772 - acc: 0.9390
Epoch 16/20
326034/326034 [==============================] - 35s 109us/step - loss: 0.1728 - acc: 0.9404
Epoch 17/20
326034/326034 [==============================] - 36s 109us/step - loss: 0.1676 - acc: 0.9420
Epoch 18/20
326034/326034 [==============================] - 39s 119us/step - loss: 0.1627 - acc: 0.9435
Epoch 19/20
326034/326034 [==============================] - 37s 112us/step - loss: 0.1574 - acc: 0.9451
Epoch 20/20
326034/326034 [==============================] - 37s 113us/step - loss: 0.1522 - acc: 0.9472
06/29/2018 07:42:45 AM __main__     INFO     Predicting test dataset... deep
06/29/2018 07:43:02 AM __main__     INFO     AUC = [0.9681608329246231]
06/29/2018 07:43:02 AM __main__     INFO     Plotting results for deep model @ data-new\deepmodel_timesplit
06/29/2018 07:44:14 AM __main__     INFO
06/29/2018 07:44:14 AM __main__     INFO     Try a shallow model with random-split data ...
06/29/2018 07:44:14 AM __main__     INFO     Constructing model - shallow
06/29/2018 07:44:16 AM __main__     INFO     Constructed model:
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_9 (Dense)              (None, 1)                 3001
=================================================================
Total params: 3,001
Trainable params: 3,001
Non-trainable params: 0
_________________________________________________________________
06/29/2018 07:44:16 AM __main__     INFO     Start training model...
Epoch 1/20
348801/348801 [==============================] - 34s 97us/step - loss: 0.3268 - acc: 0.8922
Epoch 2/20
348801/348801 [==============================] - 12s 34us/step - loss: 0.2749 - acc: 0.9053
Epoch 3/20
348801/348801 [==============================] - 8s 24us/step - loss: 0.2630 - acc: 0.9071
Epoch 4/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2558 - acc: 0.9092
Epoch 5/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2505 - acc: 0.9105
Epoch 6/20
348801/348801 [==============================] - 8s 24us/step - loss: 0.2462 - acc: 0.9106
Epoch 7/20
348801/348801 [==============================] - 9s 24us/step - loss: 0.2427 - acc: 0.9109
Epoch 8/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2398 - acc: 0.9119
Epoch 9/20
348801/348801 [==============================] - 8s 24us/step - loss: 0.2373 - acc: 0.9129
Epoch 10/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2351 - acc: 0.9136
Epoch 11/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2332 - acc: 0.9146
Epoch 12/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2315 - acc: 0.9167
Epoch 13/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2299 - acc: 0.9174
Epoch 14/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2286 - acc: 0.9190
Epoch 15/20
348801/348801 [==============================] - 10s 28us/step - loss: 0.2274 - acc: 0.9198
Epoch 16/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2263 - acc: 0.9202
Epoch 17/20
348801/348801 [==============================] - 8s 24us/step - loss: 0.2253 - acc: 0.9205
Epoch 18/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2244 - acc: 0.9209
Epoch 19/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2235 - acc: 0.9211
Epoch 20/20
348801/348801 [==============================] - 8s 23us/step - loss: 0.2227 - acc: 0.9218
06/29/2018 07:47:30 AM __main__     INFO     Predicting test dataset... shallow
06/29/2018 07:47:43 AM __main__     INFO     AUC = [0.8838864566639828]
06/29/2018 07:47:43 AM __main__     INFO     Plotting results for shallow model @ data-new\shallowmodel_randsplit
06/29/2018 07:47:45 AM __main__     INFO
06/29/2018 07:47:45 AM __main__     INFO     Try a deep model with random-split data ...
06/29/2018 07:47:45 AM __main__     INFO     Constructing model - deep
06/29/2018 07:47:45 AM __main__     INFO     Constructed model:
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_10 (Dense)             (None, 128)               384128
_________________________________________________________________
batch_normalization_7 (Batch (None, 128)               512
_________________________________________________________________
activation_7 (Activation)    (None, 128)               0
_________________________________________________________________
dropout_7 (Dropout)          (None, 128)               0
_________________________________________________________________
dense_11 (Dense)             (None, 128)               16512
_________________________________________________________________
batch_normalization_8 (Batch (None, 128)               512
_________________________________________________________________
activation_8 (Activation)    (None, 128)               0
_________________________________________________________________
dropout_8 (Dropout)          (None, 128)               0
_________________________________________________________________
dense_12 (Dense)             (None, 64)                8256
_________________________________________________________________
batch_normalization_9 (Batch (None, 64)                256
_________________________________________________________________
activation_9 (Activation)    (None, 64)                0
_________________________________________________________________
dropout_9 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_13 (Dense)             (None, 64)                4160
_________________________________________________________________
batch_normalization_10 (Batc (None, 64)                256
_________________________________________________________________
activation_10 (Activation)   (None, 64)                0
_________________________________________________________________
dropout_10 (Dropout)         (None, 64)                0
_________________________________________________________________
dense_14 (Dense)             (None, 32)                2080
_________________________________________________________________
batch_normalization_11 (Batc (None, 32)                128
_________________________________________________________________
activation_11 (Activation)   (None, 32)                0
_________________________________________________________________
dropout_11 (Dropout)         (None, 32)                0
_________________________________________________________________
dense_15 (Dense)             (None, 32)                1056
_________________________________________________________________
batch_normalization_12 (Batc (None, 32)                128
_________________________________________________________________
activation_12 (Activation)   (None, 32)                0
_________________________________________________________________
dropout_12 (Dropout)         (None, 32)                0
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 33
=================================================================
Total params: 418,017
Trainable params: 417,121
Non-trainable params: 896
_________________________________________________________________
06/29/2018 07:47:45 AM __main__     INFO     Start training model...
Epoch 1/20
348801/348801 [==============================] - 55s 158us/step - loss: 0.2989 - acc: 0.8960
Epoch 2/20
348801/348801 [==============================] - 43s 122us/step - loss: 0.2351 - acc: 0.9208
Epoch 3/20
348801/348801 [==============================] - 42s 120us/step - loss: 0.2176 - acc: 0.9272
Epoch 4/20
348801/348801 [==============================] - 40s 115us/step - loss: 0.2056 - acc: 0.9316
Epoch 5/20
348801/348801 [==============================] - 39s 113us/step - loss: 0.1969 - acc: 0.9343
Epoch 6/20
348801/348801 [==============================] - 41s 118us/step - loss: 0.1902 - acc: 0.9362
Epoch 7/20
348801/348801 [==============================] - 39s 111us/step - loss: 0.1842 - acc: 0.9381
Epoch 8/20
348801/348801 [==============================] - 39s 111us/step - loss: 0.1785 - acc: 0.9398
Epoch 9/20
348801/348801 [==============================] - 39s 112us/step - loss: 0.1728 - acc: 0.9419
Epoch 10/20
348801/348801 [==============================] - 40s 115us/step - loss: 0.1680 - acc: 0.9434
Epoch 11/20
348801/348801 [==============================] - 40s 114us/step - loss: 0.1628 - acc: 0.9448
Epoch 12/20
348801/348801 [==============================] - 40s 115us/step - loss: 0.1579 - acc: 0.9463
Epoch 13/20
348801/348801 [==============================] - 41s 118us/step - loss: 0.1540 - acc: 0.9476
Epoch 14/20
348801/348801 [==============================] - 41s 119us/step - loss: 0.1492 - acc: 0.9489
Epoch 15/20
348801/348801 [==============================] - 42s 120us/step - loss: 0.1452 - acc: 0.9500
Epoch 16/20
348801/348801 [==============================] - 42s 120us/step - loss: 0.1400 - acc: 0.9513
Epoch 17/20
348801/348801 [==============================] - 41s 119us/step - loss: 0.1359 - acc: 0.9530
Epoch 18/20
348801/348801 [==============================] - 40s 115us/step - loss: 0.1313 - acc: 0.9544
Epoch 19/20
348801/348801 [==============================] - 39s 113us/step - loss: 0.1269 - acc: 0.9555
Epoch 20/20
348801/348801 [==============================] - 39s 111us/step - loss: 0.1221 - acc: 0.9572
06/29/2018 08:01:30 AM __main__     INFO     Predicting test dataset... deep
06/29/2018 08:01:46 AM __main__     INFO     AUC = [0.9453306409610893]
06/29/2018 08:01:46 AM __main__     INFO     Plotting results for deep model @ data-new\deepmodel_randsplit
06/29/2018 08:01:48 AM __main__     INFO
06/29/2018 08:01:48 AM __main__     INFO     Plotting all results...
06/29/2018 08:01:50 AM __main__     INFO     Done!